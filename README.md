<div align="center">

# ğŸ”¬ Semantic Compression VQâ€‘VAE Pipeline

<div align="center">

![Typing SVG](https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=600&size=28&duration=3000&pause=1000&color=F07178&center=true&vCenter=true&width=900&lines=Compressing+Meaning+Not+Just+Data;Quantize+â†”ï¸+Reconstruct+via+LLMs;Revolutionizing+Semantic+Storage+%26+Retrieval)

**ğŸ“ Anywhere Data Meets Insight**  
**ğŸ¯ Mission: Semantic Compression + Generative Reconstruction**  
**ğŸ’¬ Motto: â€œBeyond Bitsâ€”Understanding.â€**

</div>

---

## ğŸ§  The Grand Vision

> **What if meaning could be compressed like bytes?**  
>  
> Imagine storing vast corpora in a few hundred bitsâ€”and then bringing sentences back to life with an LLM whisper. Thatâ€™s not sciâ€‘fi; itâ€™s what weâ€™re building.

This pipeline unites discrete VQâ€‘VAE codebooks with FAISS OPQ+PQ indexing and stateâ€‘ofâ€‘theâ€‘art LLM decodersâ€”letting embeddings become storage, and storage become sentences again. ğŸ¤–âœ¨

---

## âš™ï¸ Core Components

1. ğŸ“Š **pipeline.py**  
   **Input:** Raw text (Wikitextâ€‘2) â†’ SBERT embeddings  
   **Compression:** FAISS OPQ+PQ â†’ VQâ€‘VAE discrete latents  
   **Output:** `artifacts/sample_latents.pt`, model checkpoints & metadata

2. ğŸ¤– **decode.py**  
   **Load:** Quantized latents & config  
   **Reconstruct:** Cosineâ€‘similarity fallback or LLMs (GPTâ€‘4O, Claudeâ€‘2, Geminiâ€‘1.5â€‘Flash)  
   **Output:** Humanâ€‘readable sentences

3. ğŸ“ˆ **evaluate.py**  
   **Metrics:** Cosineâ€‘similarity, BERTScore, BLEU  
   **Report:** CSV, HTML & radar plot  
   **Visuals:** `artifacts/evaluation_radar_plot.png`

4. ğŸ“¡ **llm_callers.py**  
   Unified wrappers for each modelâ€™s APIâ€”just set your `OPENAI_API_KEY`, `ANTHROPIC_API_KEY` or `GOOGLE_API_KEY`.

---

## ğŸš€ Quickstart

```bash
# 1. Clone & config
git clone https://github.com/xbard-C42/AI-driven-semantic-compressor-VQ-VAE-Pipeline.git
cd AI-driven-semantic-compressor-VQ-VAE-Pipeline

# 2. Create vqvae_config.json
cat > vqvae_config.json << 'EOF'
{
  "device": "cpu",
  "dataset_split": "train",
  "sample_size": 10000,
  "batch_size": 64,
  "test_fraction": 0.1,
  "pq_M": 8,
  "embedding_dimension": 384,
  "vqvae_hidden_dim": 128,
  "vqvae_embedding_dim": 64,
  "vqvae_num_embeddings": 512,
  "vqvae_learning_rate": 1e-3,
  "vqvae_epochs": 10
}
EOF

# 3. Install deps
pip install -r requirements.txt

# 4. Run the 3â€‘step pipeline
python pipeline.py     # â–¶ï¸ train & save latents
python decode.py       # â–¶ï¸ reconstruct with similarity/LLMs
python evaluate.py     # â–¶ï¸ metrics & radar plot





It now works as intended and my work is likely done. 18:28 16/07/2025. It's your's now!
